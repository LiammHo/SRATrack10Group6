---
title: "logistic regression"
author: "Katie Wang"
date: "2024-07-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

library(tidymodels)  

# Helper packages
library(readr)       # for importing data
library(vip)         # for variable importance plots
```

```{r}
#importing the data
final_data <- read.csv("Data/combined_data_final2.csv") %>%
  mutate(across(where(is.character), as.factor))

dim(final_data)
```

# taking out primary productivity
```{r}
library(dplyr)

# Importing the data
final_data <- read.csv("Data/combined_data_final2.csv") %>%
  dplyr::select(-mean_PrimaryProductivity) %>% # Exclude the column
  mutate(across(where(is.character), as.factor))

# Check the dimensions of the modified data
dim(final_data)

```


```{r}
#looking at our final data and what it contains
glimpse(final_data)

final_data %>% 
  count(occurrences) %>% 
  mutate(prop = n/sum(n))
```
```{r}
set.seed(123)

splits <- initial_split(final_data, strata = occurrences)

manta_train <- training(splits)
manta_test <- testing(splits)
```

```{r}
# training set proportions by occurrences
manta_train %>% 
  count(occurrences) %>% 
  mutate(prop = n/sum(n))

# test set proportions by occurrences
manta_test  %>% 
  count(occurrences) %>% 
  mutate(prop = n/sum(n))
```
```{r}
set.seed(234)
val_set <- validation_split(manta_train, 
                            strata = occurrences, 
                            prop = 0.80)

val_set
```

```{r}
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

```{r}
lr_recipe <- 
  recipe(occurrences ~ mean_temp + mean_MixedLayerDepth + mean_salinity + mean_total_phytoplankton + mean_ph + mean_chlorophyll, data = manta_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

```{r}
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid %>% top_n(-5) # lowest penalty values
lr_reg_grid %>% top_n(5)  # highest penalty values
```

```{r}
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

```{r}
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line(color = "blue") + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot
```

```{r}
top_models <-
  lr_res %>% 
  show_best(metric = "roc_auc", n = 1) %>% 
  arrange(penalty) 

top_model <- finalize_workflow(lr_workflow, select_best(lr_res, metric = "roc_auc"))

```


```{r}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)
lr_best
```




```{r}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(occurrences, .pred_ABSENT) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```


```{r}
auc <- lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_auc(truth = occurrences, .pred_ABSENT)

print(auc)
```

```{r}
model_fit <- fit(top_model, manta_train)

# predict on the training data
train_predict <- predict(object = model_fit, new_data = manta_train) %>% 
  bind_cols(manta_train)

# predict on the testing data
test_predict <- predict(object = model_fit, new_data = manta_test) %>%
  bind_cols(manta_test)
```

```{r}
train_metrics <- train_predict %>% 
  metrics(occurrences, .pred_class)

test_metrics <- test_predict %>% 
  metrics(occurrences, .pred_class)
```


```{r}
train_metrics

test_metrics
```



```{r}
library(ggplot2)
library(vip)
library(dplyr)

# Rename and reorder the variables in the data
Importance_data <- model_fit %>%
  extract_fit_parsnip() %>%
  vip::vi() %>%
  mutate(Variable = recode(Variable,
                           "mean_temp" = "Temperature", 
                           "mean_MixedLayerDepth" = "Depth", 
                           "mean_total_phytoplankton" = "Phytoplankton", 
                           "mean_salinity" = "Salinity", 
                           "mean_chlorophyll" = "Chlorophyll", 
                           "mean_ph" = "pH")) %>%
  mutate(Variable = factor(Variable, levels = Variable[order(Importance, decreasing = TRUE)]))

# Plot with the renamed and reordered variables
Importance_plot <- ggplot(Importance_data, aes(x = Importance, y = Variable)) +
  geom_col(fill = "skyblue") +
  theme_bw() +
  scale_y_discrete(limits = rev(levels(Importance_data$Variable)))  # Flip the y-axis order

Importance_plot

```




```{r}
test_predict %>% 
  conf_mat(truth = occurrences, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") + 
  theme_minimal() +
  labs(title = "Confusion Matrix")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")
```
```{r}
# Load necessary libraries
library(sf)
library(dplyr)

# Read the shapefile
sdm_points <- st_read("Data/sdm_points_group6.shp")

# Display the current column names to understand what needs to be renamed
names(sdm_points)

# Rename columns to match the required names
# Adjust the current column names based on your actual data
sdm_points <- sdm_points %>%
  rename(
    mean_temp = thet_mn,
    mean_MixedLayerDepth = mltst_m,
    mean_salinity = so_mean,
    mean_total_phytoplankton = phyc_mn,
    mean_ph = ph_mean,
    mean_chlorophyll = chl_men
  )

# Ensure the new column names are correct
names(sdm_points)

# Assuming model_fit is your trained model and sdm_points is your dataset with coordinates
predictions <- predict(model_fit, sdm_points, type = "prob") %>% 
  bind_cols(sdm_points)

# Convert your prediction dataset into an SF object if it's not already
# This step ensures that the data is in the correct format for spatial operations
predictions_sf <- predictions %>%
  st_as_sf(coords = c("lon", "lat"))

# Replace ".pred_PRESENT" with the actual column name that has your probability of occurrence
random_rast <- predictions_sf %>%
  select(.pred_PRESENT) %>%
  st_rasterize() %>%
  aggregate(FUN = mean, by = predictions_sf) %>%
  as.data.frame(xy = TRUE) %>%
  st_as_sf()

# Plotting the results
ggplot() +
  geom_sf(data = random_rast, aes(color = .pred_PRESENT)) +
  theme_minimal() +
  labs(title = "Predicted Probabilities", color = "Probability")

```











